{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding and Positional Embedding using Recurrent Neural Network (RNN) and Transformers Architecture\n",
        "\n",
        "## Summary\n",
        "\n",
        "A Sequence to Text approach has taken to the IMDB example using Recurrent Neural Network (RNN) by training the embedding layer on own and with a pretrained word embedding layer using different sample sizes and altering text lengths from 100 to 150. Test Accuracy for different observations are mentioned in Table 1 below:\n",
        "\n",
        "**Intrepration for Table 1:** Embedding layer with masking performed well compared to Pretrained word embedding with all the different training samples (100, 200, 300, 400) by altering text length to cutoff the reviews after 150 or 300 words. Usually, it should be viceversa i.e., Pretrained word embedding should perform better compared to Embedding layer with masking.\n",
        "\n",
        "The possible reason could be as follows:\n",
        "\n",
        "\n",
        "1.   **Small Training Data:** If training data size (100, 200, 300, 400 samples) is relatively small, the pre-trained embeddings might not have enough data to adapt effectively to the specific task of sentiment analysis on movie reviews.\n",
        "2.   **Domain Specificity:** Pre-trained word embeddings like GloVe or Word2Vec are trained on massive datasets that might not be specific to movie reviews. The embedding layer trained on your own IMDB data might capture the nuances of sentiment and vocabulary specific to movie reviews better.\n",
        "3. **Masking Impact:** Padding sequences to a fixed length (150 or 300 words) with pre-trained embeddings might introduce noise, especially if many reviews are shorter. Masking removes these padding tokens, allowing the model to focus on the actual content.\n",
        "\n",
        "\n",
        "**Table 1: Comparision between Word Embedding and Positional Embedding using RNN**\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th>S.no</th>\n",
        "    <th>Model Name</th>\n",
        "    <th>Test Accuracy<br>100 Training samples,<br>150 Text length</th>\n",
        "    <th>Test Accuracy<br>200 Training samples,<br>150 Text length</th>\n",
        "    <th>Test Accuracy<br>400 Training samples,<br>150 Text length</th>\n",
        "    <th>Test Accuracy<br>100 Training samples,<br>300 Text length</th>\n",
        "    <th>Test Accuracy<br>300 Training samples,<br>300 Text length</th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Basic sequential</td>\n",
        "    <td>55.4</td>\n",
        "    <td>49.5</td>\n",
        "    <td>57.7</td>\n",
        "    <td>52.0</td>\n",
        "    <td>60.8</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Embedded layer</td>\n",
        "    <td>50.7</td>\n",
        "    <td>50.7</td>\n",
        "    <td>49.5</td>\n",
        "    <td>60.6</td>\n",
        "    <td>56.8</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td> Embedded layer with masking</td>\n",
        "    <td>61.1</td>\n",
        "    <td>59.9</td>\n",
        "    <td>61.0</td>\n",
        "    <td>62.9</td>\n",
        "    <td>61.8</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4</td>\n",
        "    <td>Pretrained word embedding</td>\n",
        "    <td>54.4</td>\n",
        "    <td>55.3</td>\n",
        "    <td>50.0</td>\n",
        "    <td>53.3</td>\n",
        "    <td>53.1</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "In addition to this, the same IMDB dataset has passed through the \"Transformer Architecture\" for both word embedding and positional embedding. Given the 100 training sample size with 300 text length performed well using RNN, transformers architecture was implememted on the sample sample size.Apparently, the Test Accuracy is higher in Transformers compared to RNN however, the positional encoding is still lower compared to the base Transformer encoder.\n",
        "\n",
        "Assuming its due to lower training sample size (100), it was increased to 1000 training samples but still the trend between positional encoding and base transformer encoder is same. Details of the observations are mentioned below:\n",
        "\n",
        "\n",
        "**Table 2: Comparision between Word Embedding and Positional Embedding using RNN and Transformers**\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th>S.no</th>\n",
        "    <th>Model Name</th>\n",
        "    <th>Test Accuracy<br>100 Training samples,<br>300 Text length</th>\n",
        "    <th>Test Accuracy<br>1000 Training samples,<br>300 Text length</th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Basic sequential</td>\n",
        "    <td>52.0</td>\n",
        "    <td>74.0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Embedded layer</td>\n",
        "    <td>60.6</td>\n",
        "    <td>71.8</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td> Embedded layer with masking</td>\n",
        "    <td>62.9</td>\n",
        "    <td>77.6</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4</td>\n",
        "    <td>Pretrained word embedding</td>\n",
        "    <td>53.3</td>\n",
        "    <td>66.5</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5</td>\n",
        "    <td>Transformer Encoder based</td>\n",
        "    <td>68.2</td>\n",
        "    <td>80.5</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>6</td>\n",
        "    <td>Positional Embedding</td>\n",
        "    <td>61.9</td>\n",
        "    <td>76.9</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "Note: This file has the execution of 100 Training Samples with 150 Text Length. Other files with training samples are attached in [this github folder.](https://github.com/cpendyal/ChaitanyaPendyalaRepository/tree/main/Transformers/IMDB%20Dataset)"
      ],
      "metadata": {
        "id": "lIcXMCaC3MnQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGDCzQ73VvK0"
      },
      "source": [
        "### Processing words as a sequence: The sequence model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9wZZIaEVvK0"
      },
      "source": [
        "#### A first practical example"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zE5-3jwI1sLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJYAfPexVvK0"
      },
      "source": [
        "**Downloading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgf6PTwiVvK1",
        "outputId": "fb02557b-de2a-45e8-aabb-daf79f6087bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  5081k      0  0:00:16  0:00:16 --:--:-- 7485k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qskkHdUfVvK1"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNX-OhpXVvK2",
        "outputId": "b876db4c-6af3-4af9-a407-23229b271ef1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 files belonging to 2 classes.\n",
            "Found 10000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "excess_dir = base_dir / \"excess\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    os.makedirs(excess_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = 5000\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1338).shuffle(files)\n",
        "    num_ex_samples = 50\n",
        "    ex_files = files[-num_ex_samples:]\n",
        "    for fname in ex_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    excess_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/excess\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls -al /content/aclImdb/excess/neg/ | wc\n",
        "!ls -al /content/aclImdb/excess/pos/ | wc\n",
        "!ls -al /content/aclImdb/val/pos/ | wc\n",
        "!rm -rf /content/aclImdb1/\n",
        "!rm -rf /content/aclImdb/"
      ],
      "metadata": {
        "id": "5A4bB94KbPqH",
        "outputId": "f651de25-e478-4426-b83a-4595effa9ead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     53     470    2702\n",
            "     53     470    2722\n",
            "  10003   90020  545011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJf8R_-eVvK2"
      },
      "source": [
        "**Preparing integer sequence datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH2ALGr_VvK2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 150\n",
        "max_tokens = 10000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KD7sSoVvK3"
      },
      "source": [
        "**A sequence model built on one-hot encoded vector sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7o7lmIRVvK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc77a70-a49b-470c-f72f-746d4486e3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 10000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                2568448   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2568513 (9.80 MB)\n",
            "Trainable params: 2568513 (9.80 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCp2KCjrVvK3"
      },
      "source": [
        "**Training a first basic sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPK-nrBnVvK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb6f69b-e06d-4320-dc9c-e54981575f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 19s 4s/step - loss: 0.6934 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5056\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 11s 4s/step - loss: 0.6878 - accuracy: 0.7400 - val_loss: 0.6931 - val_accuracy: 0.5065\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 10s 3s/step - loss: 0.6821 - accuracy: 0.8300 - val_loss: 0.6931 - val_accuracy: 0.5105\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 10s 3s/step - loss: 0.6752 - accuracy: 0.8400 - val_loss: 0.6933 - val_accuracy: 0.5118\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 10s 3s/step - loss: 0.6679 - accuracy: 0.8400 - val_loss: 0.6934 - val_accuracy: 0.5116\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 10s 3s/step - loss: 0.6580 - accuracy: 0.8500 - val_loss: 0.6938 - val_accuracy: 0.5094\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.6445 - accuracy: 0.8400 - val_loss: 0.6946 - val_accuracy: 0.5021\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 10s 3s/step - loss: 0.6102 - accuracy: 0.8100 - val_loss: 0.7130 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 11s 4s/step - loss: 0.5630 - accuracy: 0.6300 - val_loss: 0.6895 - val_accuracy: 0.5463\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 11s 4s/step - loss: 0.4978 - accuracy: 0.8800 - val_loss: 0.6833 - val_accuracy: 0.5569\n",
            "782/782 [==============================] - 17s 20ms/step - loss: 0.6843 - accuracy: 0.5538\n",
            "Test acc: 0.554\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_TIf0FHVvK4"
      },
      "source": [
        "#### Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mhW9dIVvK4"
      },
      "source": [
        "#### Learning word embeddings with the Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOaineC8VvK4"
      },
      "source": [
        "**Instantiating an `Embedding` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5htm57OVvK5"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApyfdGMMVvK5"
      },
      "source": [
        "**Model that uses an `Embedding` layer trained from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFlxPYS3VvK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b315f6a9-fc33-48b0-a8b5-f180e82768af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 10s 1s/step - loss: 0.6922 - accuracy: 0.4800 - val_loss: 0.6941 - val_accuracy: 0.5062\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 3s 886ms/step - loss: 0.6727 - accuracy: 0.7600 - val_loss: 0.6973 - val_accuracy: 0.5085\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 3s 976ms/step - loss: 0.6528 - accuracy: 0.8500 - val_loss: 0.7008 - val_accuracy: 0.5095\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 3s 1s/step - loss: 0.6299 - accuracy: 0.8300 - val_loss: 0.7049 - val_accuracy: 0.5024\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 3s 974ms/step - loss: 0.6059 - accuracy: 0.8600 - val_loss: 0.7068 - val_accuracy: 0.4976\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 3s 1s/step - loss: 0.5468 - accuracy: 0.8600 - val_loss: 0.7119 - val_accuracy: 0.4970\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 796ms/step - loss: 0.4761 - accuracy: 0.8800 - val_loss: 0.7166 - val_accuracy: 0.4966\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 3s 819ms/step - loss: 0.3532 - accuracy: 0.9500 - val_loss: 0.7499 - val_accuracy: 0.5485\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 3s 845ms/step - loss: 0.2428 - accuracy: 0.9700 - val_loss: 0.7512 - val_accuracy: 0.5686\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.1557 - accuracy: 1.0000 - val_loss: 0.8265 - val_accuracy: 0.5828\n",
            "782/782 [==============================] - 8s 8ms/step - loss: 0.6944 - accuracy: 0.5066\n",
            "Test acc: 0.507\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51XoyrViVvK5"
      },
      "source": [
        "#### Understanding padding and masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqfvUFbEVvK5"
      },
      "source": [
        "**Using an `Embedding` layer with masking enabled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USW5v2rVvK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79964125-7c99-4a29-c14e-90c896e0c6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 17s 3s/step - loss: 0.6939 - accuracy: 0.4800 - val_loss: 0.6935 - val_accuracy: 0.4934\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.6789 - accuracy: 0.8600 - val_loss: 0.6932 - val_accuracy: 0.4983\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.6652 - accuracy: 0.9600 - val_loss: 0.6928 - val_accuracy: 0.5042\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.6490 - accuracy: 0.9800 - val_loss: 0.6924 - val_accuracy: 0.5151\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.6191 - accuracy: 1.0000 - val_loss: 0.6916 - val_accuracy: 0.5263\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.5794 - accuracy: 1.0000 - val_loss: 0.6901 - val_accuracy: 0.5337\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 1s/step - loss: 0.5200 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.5516\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.4125 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.5507\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2656 - accuracy: 0.9900 - val_loss: 0.6640 - val_accuracy: 0.5987\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1340 - accuracy: 1.0000 - val_loss: 0.8027 - val_accuracy: 0.5499\n",
            "782/782 [==============================] - 13s 10ms/step - loss: 0.6593 - accuracy: 0.6110\n",
            "Test acc: 0.611\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-VPe9WRVvK6"
      },
      "source": [
        "#### Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhnKSkSkVvK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5858a18-34a8-47e7-edd5-785b937e7c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 03:35:02--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-05-03 03:35:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-05-03 03:35:04--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.31MB/s    in 4m 46s  \n",
            "\n",
            "2024-05-03 03:39:50 (2.88 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFNfneyTVvK6"
      },
      "source": [
        "**Parsing the GloVe word-embeddings file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONb4__dLVvK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40c112b-eafd-4b17-9ca1-1fbde4efff9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0hnV2fXVvK6"
      },
      "source": [
        "**Preparing the GloVe word-embeddings matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_uP7g8RVvK6"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMTQmZH_VvK6"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73MjPxZwVvK6"
      },
      "source": [
        "**Model that uses a pretrained Embedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uq2RK5KVvK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e624eb-97d7-4e36-a0e0-01693c2138cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, None, 100)         1000000   \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 64)                34048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1034113 (3.94 MB)\n",
            "Trainable params: 34113 (133.25 KB)\n",
            "Non-trainable params: 1000000 (3.81 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 32s 8s/step - loss: 0.7077 - accuracy: 0.5500 - val_loss: 0.6932 - val_accuracy: 0.5115\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 19s 6s/step - loss: 0.6852 - accuracy: 0.5700 - val_loss: 0.6926 - val_accuracy: 0.5191\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 14s 5s/step - loss: 0.6732 - accuracy: 0.5900 - val_loss: 0.6914 - val_accuracy: 0.5262\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 15s 5s/step - loss: 0.6747 - accuracy: 0.6200 - val_loss: 0.6905 - val_accuracy: 0.5303\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 19s 6s/step - loss: 0.6498 - accuracy: 0.6500 - val_loss: 0.6903 - val_accuracy: 0.5320\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 14s 5s/step - loss: 0.6345 - accuracy: 0.6300 - val_loss: 0.6895 - val_accuracy: 0.5356\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 5s 2s/step - loss: 0.6369 - accuracy: 0.6700 - val_loss: 0.6903 - val_accuracy: 0.5333\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 23s 8s/step - loss: 0.6366 - accuracy: 0.6700 - val_loss: 0.6888 - val_accuracy: 0.5378\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 5s 2s/step - loss: 0.5820 - accuracy: 0.7200 - val_loss: 0.6895 - val_accuracy: 0.5384\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 5s 2s/step - loss: 0.6075 - accuracy: 0.7100 - val_loss: 0.6915 - val_accuracy: 0.5380\n",
            "782/782 [==============================] - 14s 13ms/step - loss: 0.6882 - accuracy: 0.5436\n",
            "Test acc: 0.544\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}